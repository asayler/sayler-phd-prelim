%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}

\usepackage{url}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}

\urlstyle{same}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packed_desc}{
\begin{description}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{description}}

% --- Metadata ---
\permission{
  This work is licensed under the Creative Commons Attribution 4.0
  International License. To view a copy of this license, visit
  \url{http://creativecommons.org/licenses/by/4.0/}.
}
\conferenceinfo{CU CS Systems Prelim,}{
  Spring 2014. \\
  \crnotice{Copyright held by author(s).}
}
\copyrightetc{
  University of Colorado, Boulder \\
  \the\acmcopyr
}
\crdata{03/2014}
% --- End of Metadata ---

\begin{document}

\title{System and Data Security - A CS Systems Prelim}

\numberofauthors{1}
\author{
  \alignauthor
  Andy Sayler\\
  \affaddr{University of Colorado}
  \affaddr{Boulder, Colorado}
  \email{andy.sayler@colorado.edu}
}

\maketitle

\begin{abstract}
Security is a core component of modern computer systems. From
protecting our data to securing our communications, security across
the computing spectrum is fundamental to the manner in which we
leverage and trust computers. But the security of modern computing
systems has not come easily: it has been learned and improved slowly
over many years, sometimes at the cost of painful
lessons. Furthermore, the modern state of the art in computing
security still lea vs much to be desired. In this paper, I explore the
development of the current state of the art in computer security
focusing on four core components: cryptography, access control, file
system security, and security management. Computer security is an
inherently large topic, but these core topics provide a reasonable
basis for the modern state of computer security. In particular, I seek
to answer the question: ``How can we secure our systems and data in a
robust, comprehensive, and easy-to-use manner?''. This question in
examined from a historical perspective as well as the perspective of a
modern user with modern use cases. This paper builds on the background
work completed in my Master's Thesis~\cite{custos-masters}, further
extending my analysis of the current state of the art and
hypothesizing on future extensions to this state.
\end{abstract}

\section{Introduction}
\label{sec:intro}

We have reached the age of ubiquitous computing. There is not a facet
of our lives that is not heavily integrated with the vast computing
networks we have built and continues to expand. From our cell phones
we use to communicate to the web sites on which we mange our life's
savings to the hard drives in our laptops filled with our photos and
personal documents, computers are not only everywhere, but often at
the heart of our most intimate interactions. As such, the security of
our computing infrastructure is a foremost concern in modern computing
system design. But what is the state of the art in computing security?
And how have we arrived at this state? These are question I address in
this paper through the exploration and analysis of ten significant
publications in the field. In particular, I break my analysis of the
state of the art in computer security into four related topics:
cryptography, access control, storage security, and managing
security. These topics provide the basis of the bulk of the modern
state of the art of computer security.

On the topic of cryptography (Section~\ref{sec:crypto}), I present the
basics or modern cryptographic systems~\cite{Diffie1976}, extensions
to these systems to accommodate the diversification of
trust~\cite{Shamir1979}, and the manner in which these core concepts
can be leverage in access control
applications~\cite{Bethencourt2007}. On the topic of access control
(Section~\ref{sec:ac}), I present the basics of modern access control
models~\cite{Sandhu1996}, the ways in which these models can
incorporate cryptography to avoid the need for a trusted compute
base~\cite{Bethencourt2007}, and the manner in which various access
control schemes have been applied to modern file
systems~\cite{Miltchev2008}. On the topic of storage security
(Section~\ref{sec:fs}), I present an effort to support file system
distribution with minimal trust~\cite{Mazieres1999}, an overview of
the security mechanism employed by a range of modern file
system~\cite{Kher2005}, and the manners in which modern file system
implement access control~\cite{Miltchev2008}. Finally, on the topic of
management (Section~\ref{sec:mgmt}), I present an early effort to
standardize the basic system security primitives~\cite{Samar1996},
techniques for making security more robust and simpler for the end
user to leverage~\cite{Cox2002}, and modern efforts to unify security
primitives across multiple administrative
domains~\cite{Morgan2004}. These ten papers are by no means the
complete body of prior art, but they do elucidate the core concepts
relevant to the question at the core of this exam. References to other
relevant works will be provided where appropriate, but the bulk of my
analyses will focus on the papers listed above.

In addition to exploring the historic contributions of the work
mentioned above and the current state of the art they represent, I
also suggest possible future expansions of this state. At it's core,
this involves looking at the existing answers to the question
question: ``How can we secure our systems and data in a robust,
comprehensive, and easy-to-use manner?'' as well as proposing
potential new answers. This hypothesizing is addressed with respect to
the four core topics mentioned above within each of the relevant
sections. This hypothesizing is intended to inform potential future
research paths and projects.

\section{Cryptography}
\label{sec:crypto}

Cryptography is the basis of much of the modern computer security
landscape. This is largely because it represent a security primitive
that does not rely on trusting specific people, platforms, or systems
in order to securely function. Instead, it requires that we place our
trust in only one thing: the underlying math. This has led to the
proliferation of cryptography as the security primitive on which many
other security features are built.

\subsection{History}

Cryptography has a very long history: there is evidence of societies
employing basic cryptographic systems in order to ``secure'' writing
and messages dating back to thousands of years BCE. These early forays
into cryptography, however, lacked the sound grounding in mathematical
theory that makes cryptography so appealing today. I will thus skip
over the bulk of cryptographic history involving them.

Modern cryptography has its roots in the field of information theory
that begin to develop during WWII and advanced quickly in the post war
years. Much of information theory laid the basis for our ability to
prove that a given cryptographic algorithm requires a certain amount
of effort to crack in the absence of the ``key''. This led to the rise
of mathematically ground symmetric encryption algorithms, designed for
use with the growing availability of computers, by the early 1970s.

Symmetric cryptography algorithms function on the principle that a
single ``key'' is used to both encrypt and decrypt a message. This key
must be securely stored, or if shared, securely exchanged between
parties. Anyone with the key can decrypt the corresponding ciphertext
the key was used to create. The security of a symmetric encryption
cipher tends to be directly related to the length of the encryption
key: the longer the key, the more secure the data encrypted with it
is.

While symmetric cryptography algorithms are useful in situations where
a single actor will be both encrypting and decrypting a piece of data
(and thus can hold the required key personally), they pose a major
challenge it situations where multiple parties with to communicate
securely. In this situation, the parties must find a way to securely
communicate the required symmetric key. In the absence of other
methods, the only real way to do while avoiding both eavesdroppers and
interlopers is to meet in person and exchange the key manually. The
tediousness and lack of practicality of this task, especially in
modern digital communication systems where multiple actors may be
continents apart, led researchers to seek a better method for secure
data exchange in the absence of an inherently secure communication
channel.

The major breakthrough in solving this challenge came in 1976 with
Diffe and Hellman's publication of ``New Directions in
Cryptography''~\cite{Diffie1976}. Diffe and Hellman proposed a system
for asymmetric cryptography: a cryptography system in which one key is
used for encryption while a second, related, key is used for
decryption. When properly designed, it is computationally unfeasible
to derive one of the keys in an asymmetric cryptography system for the
other, allowing a user to publish one of their keys for the public to
consume while keeping their other key private. A member of the public
can then use the user's public key to encrypt a message that only the
holder of the private key will be able to decrypt. If all members of
the public maintain such public/private key pairs, it become possible
for any user to send any other user a message that only the recipient
an read without requiring any form of pre-secure communication
channel.

Asymmetric cryptography relies on existence of ``trapdoor'' functions
in order to operate. These functions can be quickly solved in one
direction, but are computationally complex to reverse without a
special piece of information (e.g. the 'key'). Factoring large numbers
is a classic example of a trapdoor function (and the method on which
many modern public key encryption systems are based). Factoring large
numbers is computationally difficult in cases where some piece of
secret information (e.g. one of the factors) is not known.

Diffie and Hellman proposed a potential implementation of a public key
cryptography system, although the first practical public key crypto
system came a few years latter with the invention of the
RSA~\cite{Rivest1978} algorithm. In addition to public/private key
systems, Diffie and Hellman also proposed a system for joint key
generation where two parties can negotiate a secrete key across an
insecure connection. Like asymmetric cryptography, such a system can
also be used to bootstrap secure communications across an insecure
connection by allowing two parties to derive a secret key that can
then be used to facilitate further secret communication using a
symmetric encryption algorithm.

Diffie and Hellman also introduce the concept that asymmetric
encryption can be used to build the other key cryptographic primitives
we have come to rely upon: cryptographic verification and
cryptographic authentication. Cryptographic verification (also called
a cryptographic ``signature'') is essentially the reverse of
asymmetric encryption: instead of a member of the public using another
party's public key to encrypt a message that only the target party can
read, the target party uses their private key to encrypt a message
that the public can then decrypt using the target's public key. Since
only the target has access to the private key, and is thus capable of
generating such a message, the target can ``prove'' that a given
message comes from them and that it has not been altered in
transit. Just an asymmetric encryption also gives us cryptographically
secure signatures, cryptographically secure signatures can give us a
form of cryptographically secure authentication. If a user generates a
signed message saying ``I am John'' and sends it to an authentication
server, the server can verify that the message signature is valid by
checking it with John's public key, thus authenticating John. The
server need only have a list of public keys for each user and can then
use the fact that only the given user has access to the corresponding
private key and is thus the only one capable of generating a signed
message on the user's behalf as the basis of user authentication.

Beyond the rise of public key cryptography, one of the other major
cryptographic breakthroughs of the last fifty years was the invention
is cryptographically secure secret sharing schemes. In particular, Adi
Shamir (the 'S' for RSA~\cite{Rivest1978}) proposed a practical and
robust secret sharing scheme in his 1979 paper ``How to share a
secret''~\cite{Shamir1979}. In this work, he lays out the basics of
what came to be known as Shamir Secret Sharing: a method for splitting
a piece of information up into two or more pieces in a manner such
that holders of any subset of the pieces cannot infer any information
about the pieces they do not hold or the original information block as
a whole. Shamir Secret Sharing allows a user to divide a piece of data
D into N pieces of which K or more pieces can be used to recompute the
original value of D. A user with fewer than K pieces, however, has no
more information about the value of D than a user with no pieces. This
system provides a highly useful method for distributing information
amongst multiple parties or systems in situations where no single
system can be fully trusted.

Shamir Secret Sharing, unlike all known asymmetric encryption
techniques, does not rely on computationally complexity of it's
security. Instead, it is fundamentally secure based on basic
information theory principles. Thus, unlike computationally secure
systems like RSA, Shamir Secret Sharing can not be broken regardless
of the amount of computational power one posses. Shamir Secret Sharing
functions on the basis of defining a polynomial of degree (K-1) over a
finite field with the Data D encoded as the first order-zero term. N
points are then selected from this polynomial and distributed to the
participants. Since K points (but no fewer) will uniquely identify the
original polynomial, and thus allow the deviation of D, K users must
combine their pieces in order to re-compute D.

Shamir Secret Sharing (and related systems) are useful in a wide range
of situations where one needs to distribute trust across multiple
entities. In particular, secret sharing techniques are leveraged in
some cryptographically-based access control systems like that
described in~\cite{Goyal2006}. Such systems will be discussed further
in Section~\ref{sec:ac}.

\subsection{State of the Art}

Both symmetric and asymmetric encryption systems have a place in the
modern security landscape: symmetric systems for their performance and
resistance to cryptanalysis and asymmetric systems for their
avoidance of the key exchange problem.  Often symmetric and asymmetric
cryptography are used together, each system playing to its
strength. Symmetric systems are good at quickly and securely
encryption data, making them appropriate for the core of an encryption
system. Symmetric ciphers, however, suffer from a lack of natively
secure method for exchanging the required encryption key. This is
where asymmetric cryptography and related secure key exchange systems
come in handy. These systems provide the basis for securely exchanging
data over insecure channels and they can be thus used to bootstrap
symmetric encryption systems by facilitating the secure exchange of a
symmetric encryption key which can then be used to encrypt the
underlying data. Such systems are common in many modern protocols like
SSL, TLS, PGP, and SSH.

Modern symmetric encryption ciphers like AES (Rijndael), Twofish, or
Camellia are well-established, fast, and secure methods for encrypting
data. Symmetric encryption systems are the preferred means of
encrypting files, hard disks, and other large chunks of data due to
their speed and relative simplicity of implementation. They are also
useful for encrypting streams of data in communication protocols like
TLS/SSL or SSH. They tend to be well understood, and are generally
considered highly secure (at least the well vetted ones). The strength
of a given symmetric cryptography is directly related to the length of
the associated encryption key. Common key lengths generally considered
secure today include 128-bit keys, 256-bit keys, 384-bit keys, and
512-bit keys.

Modern asymmetric encryption systems include systems like RSA,
ElGamal, ECDH, ECDSA, or ECIES. These systems are all built atop
various one-way trapdoor functions. RSA is based on
prime-factorization functions, ElGamal is based on discrete logarithm
functions, and ECDH, ECDSA, and ECIES are all based on various
elliptic curve functions. It is often useful to mix cipher suites
relying on different trapdoor functions as a hedge against an
efficient solution to any of the underlying function families being
discovered, thus rendering the encryption using such a family
obsolete. Within a given family the security of an asymmetric cipher
is related to the length of the keys in a key pair: longer keys are
more secure. Standard key lengths for asymmetric keys depend of the
family of functions be used. In prime-factorization based system like
RSA or discreet-log systems like ElGamal, key lengths of 1024-bits,
2048-bits, and 4096-bits are all common (with 2048 considered to be
best practice for general data and 4096 considered to be best practice
for highly sensitive data). Elliptic curve based systems tend to use
shorter keys: recommended sizes range from 160-bits to 512-bits
(similar to symmetric key lengths).

\subsection{Future Extensions}

While both symmetric and asymmetric cryptography are largely settled
art at this point, there are a variety of challenges yet to be solved
in the cryptography research realm. One of the constant threats to the
settled cryptographic art is that of a major breakthrough in solving
the currently ``hard'' problems used as the basis to a variety of
trapdoor functions. One of the proposed methods for solving the
existing trapdoor functions more quickly than currently possible is to
use quantum computing approaches. While no practical attacks on
existing cryptographic systems using quantum approaches are known,
there are certainly researchers working on such approaches. There is
also work being done in the opposite direction to try to leverage
quantum techniques to build cryptographic systems that would be
effectively unbreakable due to the underlying quantum limitations of
our universe. Such techniques often involve leveraging ``observer''
effect to detect eavesdropping on a communication channel, allowing
the users to regenerate keys until they complete the negotiation
unobserved. Again, however, there are no known practical
implementations of quantum cryptography at this time.

One of the other major challenges to cryptography today is the
reliance on good sources of randomness to generate the secure
cryptographic keys used in berths symmetric and asymmetric encryption
systems. Finding and harnessing good randomness is challenging since
it depends on having access to an sufficient amount of
entropy. Traditionally, computing systems often leverage user
interactions (e.g. mouse movements, key strokes, network interrupts,
etc) as sources for OS-maintained entropy pools. The rise of cloud
computing systems, however, posses challenges to using such sources:
in the cloud, systems often run in highly homogeneous environments
with few ``random'' user interactions. Coming up with secure ways to
derive entropy and provide good randomness is such environments is a
topic of active study. The importance of randomness to modern
computing systems requires us to come up with secure ways of gathering
and distributing entropy. Potential solutions range from building
secure Entropy-as-a-Service systems that gather entropy from sources
where it is abundant and redistribute it to source where it is scarce
to using entropy-generating hardware that relies on unpredictable
physical phenomena (e.g. atomic decay events) to derive randomness.

Finally, while strong cryptographic systems are well understood, there
are a myriad of situations where cryptographic best practices are
ignored or where cryptographic systems are improbable used, leading to
security failings. Many of these issues can be linked to usability
challenges within cryptographic systems: if a system is difficult to
use or challenging to deploy, it will often go unused or wind up
misconfigured and insecure. One of the major usability challenges
present in all current cryptography systems is the management of
private/secret keys. Keeping such keys secure while also enabling
their use across a range of modern multi-user, multi-device, ephemeral
resource use cases is extremely challenging and lacks a standard
solution at this time. My previous master's thesis work,
Custos~\cite{custos-masters}, proposes a potential solution to the
secret management problem and remains an area of active research.

\section{Access Control}
\label{sec:ac}

Over the years, we have developed a range of access control
techniques. All of these techniques share a common goal: controlling
access to a specific system, resource, or piece of data. Access
control has to key components: authentication and
authorization. Authentication is used to establish the identity of an
actor. Authorization then leverages this identification as the basis
of granting or denying specific capabilities to the actor. This
section will discuss historic access control techniques, the current
state of the access control art, and potential future access control
additions.

\subsection{History}

Computer-based access control systems have been with us since the
earliest multi-user (e.g. time sharing) operating systems became
popular in the 1970s and 1980s. Early access control systems were
primarily focused around the Unix model of access control: users,
groups, and read/write/execute file-level permissions. Authentication
in these early systems was generally limited to username:password
combinations, the mechanisms of which were hard coded into the
\texttt{login} program. Each user was a member of one or more groups
and each file had a owner and group. The three file permissions, read,
write, and execute, were granted on the basis of a user's relationship
to a given file: either the user was the file owner, the user was a
member of the file group, or the user was neither of these
things. This model is generally flexible, and continues to be used
today as the core access control model in many Unix-like operating
systems (e.g. BSD, Linux, OSX, etc).

Access Control List (ACL) based schemes gained prominence in 1990s and
were popularized by the Windows NT family of operating systems. ACLs
extend the permission model beyond the basic Unix file permissions to
include a wider range of file (e.g. read, write, delete, create, etc)
and system-level permissions (e.g. shutdown, connect to network,
etc). ACLs are associated with specific system objects (e.g. files,
folders, OS subsystems, etc) and map a user or group to a list of
permission that user or group possess. They generalize the Unix access
model to accommodate a wider range of permissions and mappings between
permissions and actors. ACL-based systems have been integrated into
many modern Unix-like operating systems as an optional extension
beyond the tradition Unix permissioning scheme.

Exiting access control schemes are often groups into one of two
classes: Mandatory Access Control (MAC) systems or Discretionary
Access Control (DAC) Systems. While the lines between these two
approaches are occasionally blurred, the basic difference between the
two lies in which actors within a system have the ability to
grant/extended permissions to other actors. In MAC system, all
permissions are set by the system administrator and users have no
ability to change these permissions themselves or transfer permissions
to other users. DAC systems, in contrast, give users the ability to
set their own permissions on objects they own or create, and to
transfer these permissions to other users. A MAC-based system can be
thought of as similar to a DAC system where the system administrator
owns all files and never transfer this ownership to any other
user. Traditional Unix access control systems as well ACL access
control systems can generally be used in either MAC or DAC based
systems. MAC systems are generally preferred in high security
environments where the centralized management models they offer lead
to tighter control over data. DAC systems are more common in general
purpose systems where the extra flexibility they offer reduces the
administrative burden. Most Unix-like systems are DAC systems by
design, but extensions (e.g. SELinux) can be used to add MAC
properties to these systems.

Many of the early access control systems pose a host of manageability
challenges. How do you coordinate the permissions of thousands of
users across millions of objects? How do you revoke permissions for a
defunct user? Or add a new user? To cope with many of these
challenges, Sandhu, Coyne, Feinstein, and Youman proposed the concept
of Role-Based Access Control Models in their 1996 paper by the same
name~\cite{Sandhu1996}. Role-Based Access Control (RBAC) inserts an
additional layer of indirection between users and permissions. In an
RBAC system, users are assigned to one or more roles. Each role is
then assigned one or more permissions. This model simplifies
management by separating permission assignment for individual users,
allowing permissions to be assigned on the basis of specific positions
or duties within an organization and mapped to specific roles. Users
are then assigned to these roles on the basis of whether or not they
hold a specific positions or are required to perform a specific
duty. Thus, adding or removing users does not require any modification
to permission mappings, only role mappings. Likewise, adding or
removing permissions does not require modifying use mappings, only
role mappings as well. \cite{Sandhu1996} describe 4 classes of RBAC
systems: $RBAC_0$ (the base model), $RBAC_1$ (the base model with the
addition of role hierarchies and inheritance), $RBAC_2$ (the base
model with the addition of constraints), and $RBAC_3$ (the combination
of $RBAC_1$ and $RBAC_2$).

The primary limitation of all of the all of the access control model
mentioned thus far is their reliance on a trusted arbiter for
enforcement: generally this trusted arbiter is the operating system or
some other underlying program in charge of exposing the access control
system. This means that the security of any of the access control
systems is only as good as the security of the system enforcing them
(e.g. the security of the OS). Thus, if the underlying OS or program
is compromised, the access control system falls apart. Likewise,
anyone in control of the underlying OS or program automatically gains
full control over the access control system. This is an acceptable
limitations in many situations, especially those based on a centrally
managed system with existing physical security and administrative
safeguards in place. But in distributed system or other systems where
physical and management control in not guaranteed (e.g. the Cloud), a
more robust system that lacks this ``trusted arbiter'' requirement is
desirable.

To overcome the need for a trusted enforcement mechanism in access
control systems, researchers have turned to cryptographically-based
access control systems. As mentioned in Section~\ref{sec:crypto},
cryptographic-based system require no trust in external systems, only
in the underlying math. Sahaie, Waters, et. al. propose several
cryptographically-based access control systems in their 2006
paper~\cite{Goyal2006} and it's 2007
follow-up~\cite{Bethencourt2007}. They're systems are all based on the
concept of Attribute-Based Encryption (ABE) schemes. ABE schemes allow
a user to encrypt a document in manner such that the access control
rules associated with the document are part of the encryption process
itself. Thus, in order to decrypt/access a document, a user must
satisfy one or more cryptographically guaranteed access control
attributes. \cite{Goyal2006} allows user to encrypt documents that can
only be decrypted by users possessing specific attribute polices
encoded in their keys. \cite{Bethencourt2007} extended this concept to
allow documents to be encrypted with a full access control policy tree
embedded in the encryption process directly allowing only users who's
private keys meet a generalized set of requirements to access the
documents. Both these systems allow the construction of access control
systems that do not require any trusted arbiter to regulate access to
objects. Instead, the regulation in enforced by the underlying
encryption.

\subsection{State of the Art}

Today, the state of the are in access control differ widely based on
the system and application. As mentioned, many Linux and Unix-like
systems still use basic Unix access control primitives largely
unchanged over the last 20 to 30 years. This is largely due to the
fact that these systems are ``good enough'' for many single-user and
small group environments and the administrative burden of shifting to
a more advanced system is simply not worth the effort. Most Linux
systems today do support extended file ACLs, as well as system-level
ACLs exposed by systems like PolicyKit. These systems allow users to
move beyond the limitations of a pure Unix-like, file-centric access
control scheme. That said, many users never need to deal with these
items for the majority of day-to-day use cases.

Windows-based operating systems make extensive use of ACL-based access
control schemes. While, these schemes are useful in the Windows-domain
environments used by most large corporations, many home and leisure
Windows users never have any need to interact with file or system
level ACLs. Modern Windows systems combine RBAC concepts with ACL
concepts by allowing administrators to define role-based ``groups''
that can then be used with specific permission assignments. Most
stand-alone access control system bundled with specific systems
(e.g. content management systems, etc) also take ques form both RBAC
and ACL access control models. While these system do help to reduce
the management burden of large systems, they are often prone to
misconfiguration, the occurrences of which lead to many of the
security breaches that occur today.

Cryptographically-based access control schemes remain largely an
academic novelty at this point. I am aware of no commercially or
generally deployed software that leverage ABE or similar
cryptographically-based access control schemes as the basis of their
access control models. None the less, the increasing need to robust
access control schemes that can be used across a range of untrusted
infrastructure is an increasingly important need in the Cloud-based
computing world. Thus, it is possible that we will see an increase in
the practical deployment of these systems in the near future.

Outside of permission-side access control models, there have also been
advances in the authentication side of access control. Most modern
access control systems support authorization primitives far more
complex than basic username:password combinations. Several such
systems are discussed in more detail in Section~\ref{sec:mgmt}. None
the less, the vast majority of user authorization systems are still
password based. To overcome the well known security deficiencies of
user passwords, multi-factor authentication schemes are staring to
gain prominence on high value target systems (e.g. email accounts,
bank accounts), but such systems are largely still optional and are
not in wide use amongst the general population.

\subsection{Future Extensions}

One of the major limitations to most access control system today is
their lack of global name space or rules: access control rules are
currently scoped to the system and administrative domain in which they
are created, with little to no support for wider, globally-enforceable
ruled. This creates a host of challenges implementing robust access
control schemes across our modern multi-user, multi-device, loosely
managed environments. As a basic example, tradition Unix file
permissions are useless when used on portable media like USA flash
drives since user IDs, groups, and permissions are all scoped to the
local machine and do not extend to other machines on which you might
wish to access the portable media. Cryptographically-based access
control schemes like ABE are a step toward solving this problem by
removing the need for a trusted host system. Still, such systems still
pose many of the same challenges other cryptographic systems have:
namely how to manage and control access to the underlying private keys
in a secure yet globally accessible manner. In addition, a variety of
system-specific distributed access control schemes will be discussed
in the context of the file systems to which they apply in
Section~\ref{sec:fs}. But few of these systems are generalized enough
for use on a wide multi-system scale. Proving a secure, manageable,
and usable access control systems that can operate on a global scale
across a variety of distributed devices largely remains an open
problem, and finding a solution will continue to increase in
importance as our use cases continue to demand an increasingly global
and distributed perspective.

Beyond globally usable access control schemes, most access control
system today have a fairly clear delineation of authentication and
authorization. While this division makes since from a separation of
duties standpoint and fits well into tradition access control schemes,
it can also limit the expressiveness of an access control system. For
example, strictly separating authentication and authorization makes it
difficult to set up access control rules that depend on more than a
single user's identity (e.g. time dependent, etc) and also make it
difficult to operate in situations whee the concept of a single
``user'' is not well defined (e.g. anonymous
systems). In~\cite{custos-masters}, I explored relaxing the separation
between authentication and authorization to build a more expressive
access control scheme. Schemes like ABE also blur the liens between
authentication and authorization in the name of increased
expressiveness. How to strike a correct balance between a proper
separation of authentication and authorization duties and a high level
of expressiveness remains an open problem. Building expressive systems
that are also easy to reason about, manage, and maintain is a relevant
topic to future access control work.

\section{Storage Security}
\label{sec:fs}

Much of the work we perform on computers today is highly data
centric. As such, the protection and control of our data is a core
goal within the realm of computer security. The previous two sections
explored ways to protect data cryptographically and via various
generalized access control models. In this section, I'll look at data
protection schemes built into storage systems directly.

\subsection{History}

Early storage and file system technologies often simply neglected
security, lacking robust encryption and access control primitives. As
mentioned in Section~\ref{sec:ac}, the rise of multi-user operating
systems like Unix mandated the creation of basic file-system access
control schemes. Thus we gained the previously mentioned Unix file
access control and permissioning scheme as part of the virtual file
system level inherent in all legacy and modern Unix-like operating
systems. As previously stated, however, this system has a number of
limitations: it supports only a single, basic access control model
(owner, group, R/W/E permissions), it requires a trusted system for
enforcement, and it is strongly coupled to a local system. Systems
like NFS attempt to extend Unix file security semantics beyond the
local machines and to remote sharing of files, but even these systems
are limited to singular administrative domains and trusted systems.

The Windows NT file system access control model (implemented via the
NTFS file system) extends the flexibility of the traditional Unix
model by adding support for more expressive ACLs. These both allow the
control of additional permissions (e.g. delete, create directory, etc)
as well as more expressive user to permission mappings beyond the
basic owner/group/other Unix model. Furthermore, the Windows NT model
has the ability to delegate user authentication to a local Domain
Controller (DC) capable of centrally managing all users from a single
location. This expands the ability to control file access beyond the
users associated with the local system to the users associated with an
entire administrative domain. Still, this system still has many of the
same limitations as the Unix model: the requirements for a trusted
system for enforcement and the tight coupling to the local
administrative domain.

The rise of the Internet as a reliable and high speed system for
connecting multiple machines across the world as well as the move
toward Cloud computing models where computational resources are
outsourced to dedicated providers has increased the demand for secure
storage systems capable of spanning multiple systems and domains. In
order to overcome the limitations posed by traditional file system
security models accommodate the modern multi-user, multi-system use
cases, researchers have proposed a number of newer systems. These
systems try to address one or more of the limitations mentioned
above. Some of them employ cryptographic security models to overcome
the need for a trusted enforcement system. Others are designed to
extended access control semantics beyond the local machine to large
local networks or even the global internet. Still others explore the
use of novel access control model more expressive then Unix
permissions or Windows NT ACLs. Many system combine more than one of
these approaches to build a fully featured next generation secure
storage system.

Kher and Kim provide a survey of the various approaches to securing
distributed storage systems in their 2005 paper ``Securing Distributed
Storage: Challenges, techniques, and Systems''~\cite{Kher2005}. In it,
they discuss the security models of various storage systems, sorting
such systems into basic networked file systems, single-user
cryptographic file systems, and multi-user cryptographic file
systems. As previously mentioned, basic networked file systems rely on
trusted systems and administrators for the enforcement of security
rules. Examples of such systems include the Sun Network File System
(NFS), the Andrew File Systems (AFS), and the Common Internet File
System (CIFS/SMB). All of these systems are designed for use within
local administrative domains and do not scale well to global,
loosely-coupled distributed systems. To deal with the scalability
issues, researchers have built system like SFS (discussed below) or
OceanStore which aim to reduce the administrative burden of large
scale distributed file systems. All of these systems, however, rely on
some degree of system or administrator trust. In order to accommodate
situations where users do not wish to place trust on the underlying
system or remote servers, Kher and Kim discuss a handful of
cryptographically-secure file systems. The best of these systems offer
end-to-end cryptography, meaning that data is encrypted and decrypted
on the client side and the server never has access to the unencrypted
data.  Systems like the Cryptographic File Systems (CFS) provide basic
single-user end-to-end file encryption. While end-to-end encryption is
a powerful security model for enabling secure options atop untrusted
systems, it does pose challenges with respect to multi-user,
multi-device use cases since ti requires all clients to have access to
private cryptographic credentials in order to effectively read or
write files. In order to support both end-to-end encryption and
multi-user scenarios, more complex systems like SiRiUS, Cephus, or
Plutus.

Miltchev, et. al. provide a second survey of success control
techniques across a verity of distributed file systems in their 2008
paper ``Decentralized Access Control in Distributed File
Systems''~\cite{Miltchev2008}. They present a framework for analyzing
the suitability of distributed file system for modern multi user,
multi domain use cases by analyzing five underlying file system
qualities: authentication, authorization, granularity, delegation, and
revocation.  Miltchev, et. al. suggest that any secure large scale
files system must successfully address the functionality of all five
of these factors across multiple administrative domains in order to be
an effective multi user, multi domain file system. In addition to the
systems discussed in~\cite{Kher2005}, Miltchev, et. al. also introduce
systems like Truffles, Bayou, WebFS, CapaFS, DisCFS, WebDAVA, and
Fileteller as examples of systems that attempt to support multi
domain, multi user, globally-accessible use cases. Miltchev,
et. al. reach the following conclusions regrading successful secure
multi-user, multi-domain file systems: the use of public-key
cryptography for user authentication is an effective way to support
autonomous delegation, capability-based access control systems tend to
lack support for auditing and accountability, ACL-based access control
systems pose scalability challenges when used across administrative
domains, and revocation and user autonomy are often at odds.

A good example of a modern multi-user distributed file system with
many of the desirable qualities discussed previously is SFS, described
by Mazi\`{e}res, et. al. in their 1999 paper ``Separating Key
Management form File System Security''~\cite{Mazieres1999}. In this
paper, they describe the design and implementation of the SFS file
system. SFS is unique in that it is designed for global-scale file
system operations and leverages cryptographic security while avoiding
the need for tightly specified key management infrastructure. Such
infrastructure often adds a large administrative burden, limiting file
system expansion beyond a single administrative domain on the basis of
management complexity. SFS achieves it's goals through the use of
self-certifying file names: file names that encode the public key of
remote file servers into the file path itself. Self-certifying path
names allow SFS clients to bootstrap a cryptographically secure
communication channel to nay remove server without requiring any large
scale key-management infrastructure. SFS leverages authentication
servers to verify users on the basis of asymmetric key pairs and uses
user agents to help reduce the usability burden this might otherwise
impose (see Section~\ref{sec:mgmt} for more details on agents). SFS is
built atop the NFS distributed file system, and thus uses an a
Unix-based access control model to regulate file and directory
access. SFS's cryptographic capabilities help it to overcome the need
for a fully trusted compute base for the enforcement of its security
model. Extensions to SFS like GSFS help move SFS beyond the
limitations of a single administrative domain to accommodate a more
general global file system. SFS does not however provide full
end-to-end encryption, so it still requires some trust of the
underlying infrastructure and administration techniques.

\subsection{State of the Art}

As mentioned in Section~\ref{sec:ac}, the basic Unix and Windows NT
final system security models are by far the most widely deployed file
system security approaches. Where multi-system distributed file
systems are concerned, NFS, AFS, and CIFS are the de facto
standards. As discussed, all of these systems lack support for
end-to-end cryptographic security, global access, and inter-domain
sharing. They are the standard not because of their rich, modern
feature sets (which they lack), but simply because they are adequate
for many of the standard single user or single domain use cases and
replacing them would be a larger burden than it is worth. Again, these
are not ideal systems, they are simply ``good enough'' for many users,
and thus remain the standard.

In many instances, the tradition systems listed above are not so much
threatened by the more advanced research systems previously discussed
(e.g. SFS, Bayou, Plutus, etc) as they are threatened by the growing
proliferation of cloud-based distributed storage service like Dropbox
or Google Drive. Today, when users wish to share files across
administrative boundaries (or even within these boundaries) or wish to
sync files across multiple devises, they often do so using a dedicated
file sharing/syncing service based in the cloud. These services tend
to have highly polished user interfaces and overcome many of the
single-domain, single-system restrictions of more traditional
approaches like NFS or CIFS. While polished and easy to use, these
systems do require placing full trust in the cloud providers that
operate them. While most users seem willing to trade trust and privacy
for the features these service provide, there is a growing
understanding (a la Edward Snowden and our friends at the National
Security Agency) of the privacy risk offloading data to the cloud
involves. While these risks don't seem to deter a majority of users
(at least this time), they do make such service strictly off limits
within certain high security and regulatory realms. To date, no of the
major cloud storage services offer access to end-to-end encryption
schemes that would allow users to store they data in the cloud while
also minimizing their need to trust a given cloud service
provider. The reasons behind the lack of a cryptographically secure
cloud service seems to be a combination of lack of user demand,
usability challenges such a system would entail, and the conflicts
such a system would pose for many cloud providers data0-mining based
business models.

While cryptographically secure distributed or cloud-based file systems
are not in wide spread use, there are a growing number of
cryptographic secure local file systems worth mentioning. On Linux,
systems like eCryptfs provide an overlay file system capable of
performing transparent encryption on a subset of the systems file
tree. Alternatively, systems like dm-crypt provide block-level
encryption suitable for full-disk encryption schemes. Using a local
disk encryption system helps to guard against data loss or compromise
in the event that a storage device falls into an advisories
hands. Such systems are particularly useful on mobile computing
devices like laptops or tablets since these devices tend to be more
prone to theft or loss. Systems like BitLocker on Windows or FileVault
on OSX can be used to provide similar features. Even most modern
SSD-based hard disks tend to offer on-board encryption features to
help protect them in the event of a loss. All of these systems tend to
be fairly user friendly and/or transparent. They often tie encryption
keys to a user's login password, allowing a user to encrypt their
system without requiring any more effort than they would exert during
a normal password-based login process. This approach, however, does
mean that such systems are not useful for protecting files that can be
stolen while a user is actively logged into a system (e.g. via a
malicious program). They only provide protection when a system is
powered off, locked, or similarly put into a non-active state.

\subsection{Future Extensions}

There is a lot of work to be done in order to make cryptographically
secure, distributed file systems a day to day reality for most
users. While theoretical research systems exist that can provide many
benefits over the current status quo, many of these systems introduce
usability challenges that make them unsuitable for the average
user. As mentioned in previous sections, many of these usability
challenges can be directly linked to the problems that arise managing
private cryptographic keys across our modern multi-device, multi-user
use cases. Usability is the major hurdle preventing cryptographically
strong distributed storage systems form entering the
mainstream. Proving systems that can offer cryptographic security
while also achieving the kind of intuitive usability provided by
service like Dropbox or Google Drive needs to be a major area of
research if we wish for such systems to provide a benefit other than
supporting tenure-track progress with the ivory tower.

The rise of cloud-based services is only going to increase the demand
for secure storage systems that can support multi-user, multi-device,
multi-domain user cases while also minimizing the need to trust cloud
providers. Providing such a system will open up the growing range of
cloud service to an even wider audience, and will help to keep users
in control of their data even as we increasingly outsource our
computer resources. Custos~\cite{custos-masters} was one attempt at
building a component of such a system. Other attempts and further
research are required if we're serious about closing the cloud vs
trust divide.

\section{Managing Security}
\label{sec:mgmt}

\subsection{History}

\subsection{State of the Art}

\subsection{Future Extensions}

%% Moving beyond basic authentication primitives, there are also a range
%% of existing authentication protocols and
%% standards. Kerberos~\cite{Kohl1994, Neuman1994} was an early and
%% widely deployed authentication system. It aims to provide secure
%% authentication over untrusted networks, as well as to allow
%% token-based single-sign-on access across multiple sites and
%% services. Kerberos is still used widely today as part of the Microsoft
%% suite of operating systems and in a number Linux and Unix
%% environments. Similarly, SAML (Security Assertion Markup
%% Language)~\cite{saml}, SASL (Simple Authentication and Security
%% Layer)~\cite{sasl} are standardized formats for exchanging
%% authentication and authorization data. SAML is the basis of
%% authentication systems like Shibboleth~\cite{shibboleth, Leandro2012}
%% whose aim is to create a standardized federated authentication system
%% for use across the Internet. Systems like OAuth~\cite{oauth},
%% OpenID~\cite{openid}, or Persona~\cite{persona} operate under a
%% similar principle, allowing users to designate a federated Cloud-based
%% identity providers who can be used to authenticate the user to a range
%% of disparate web services.

%% PAM~\cite{linux-pam, openpam} is a framework for integrating a variety
%% of authentication primitives and systems in an application. PAM is
%% used by Linux and a variety of other POSIX operating systems as the
%% basis for a flexible user login authentication system. PAM exposes a
%% standardized API for integrating various authentication technologies
%% into the a generalized authentication framework.


\section{Conclusion}
\label{sec:conclusion}

Global Accessibility and Control

Multi-User/Multi-Device/Multi-Domain

Trust Management

Usability

% Bibliography
\bibliographystyle{acm}
\bibliography{prelim}

\end{document}

%%  LocalWords:  BCE Diffe Hellman's Diffie Custos AFS CryptoCache et
%%  LocalWords:  OceanStore SFS Plutus Adi Shamir ElGamal ECDH ECDSA
%%  LocalWords:  ECIES RBAC ACL DAC SELinux Sandhu Coyne Feinstein al
%%  LocalWords:  Youman Sahaie PolicyKit NTFS Kher CIFS SMB CFS WebFS
%%  LocalWords:  SiRiUS Cephus Miltchev CapaFS DisCFS WebDAVA Mazi dm
%%  LocalWords:  Fileteller SFS's GSFS Snowden BitLocker FileVault
%%  LocalWords:  SSD
